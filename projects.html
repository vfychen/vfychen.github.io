<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Projects</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1><a href="https://web.cs.ucla.edu/~fychen/projects" target=&ldquo;blank&rdquo;>Projects</a> | <a href="https://web.cs.ucla.edu/~fychen/" target=&ldquo;blank&rdquo;>Homepage</a></h1>
</div>
<h2>Summary</h2>
<p>I have a diverse academic and industrial background with 4+ years of applied research experiences, including Multimodal AI <sup><a href="https://github.com/Eurus-Holmes/Awesome-Multimodal-Research">[0]</a>,<a href="#1">[1]</a>,<a href="#2">[2]</a>,<a href="#3">[3]</a>,<a href="#4">[4]</a>,<a href="#a">[a]</a>,<a href="#b">[b]</a></sup>, Model Compression &amp; Efficient ML <sup><a href="#5">[5]</a>,<a href="#8">[8]</a>,<a href="#9">[9]</a>,<a href="#b">[b]</a></sup>, Graph Neural Networks (GNNs) <sup><a href="#5">[5]</a>,<a href="#2">[2]</a>,<a href="#6">[6]</a>,<a href="#7">[7]</a></sup>, Robustness &amp; Trustworthy AI <sup><a href="#1">[1]</a>,<a href="#6">[6]</a>,<a href="#9">[9]</a>,<a href="#a">[a]</a></sup>, Computer Vision <sup><a href="#9">[9]</a>,<a href="#10">[10]</a>,<a href="#11">[11]</a></sup>, NLP <sup><a href="#1">[1]</a>,<a href="#3">[3]</a>,<a href="#4">[4]</a></sup>, AI for Healthcare <sup><a href="#2">[2]</a>,<a href="#6">[6]</a>,<a href="#7">[7]</a></sup>, Data Augmentation <sup><a href="#9">[9]</a>,<a href="#11">[11]</a></sup>, Large-Scale ML <sup><a href="#5">[5]</a>,<a href="#8">[8]</a>,<a href="#b">[b]</a></sup>, Auto ML <sup><a href="#9">[9]</a></sup>, Speech &amp; Audio <sup><a href="#4">[4]</a></sup>, and solid software engineering experiences in algorithm design, data structures, problem-solving, and complexity. I’m also active in academics and have served as a reviewer for many top-tier AI/ML conferences, like ICLR, ICML, NeurIPS, etc. 
</p>
<p>More generally, my interested topic lies widely in the scope of 
</p>
<ul>
<li><p>1). Multimodal Machine Learning: representation, alignment, translation, fusion, and co-learning of heterogeneous data; 
</p>
</li>
<li><p>2). Applied Machine Learning: real-world applications in language, vision, speech, robotics, education, and self-driving; 
</p>
</li>
<li><p>3). AI + X: AI for healthcare, finance, climate, agriculture, astronomy, art, and VR (Metaverse).
</p>
</li>
</ul>
<h2>Experiences</h2>
<ul>
<li><p>Visiting Student Researcher, <a href="https://www.salesforceairesearch.com/" target=&ldquo;blank&rdquo;>Salesforce Research</a>, 2022
</p>
</li>
<li><p>Machine Learning Intern, <a href="https://www.aboutcoupang.com/" target=&ldquo;blank&rdquo;>Coupang</a>, Jun. 2022 – Sep. 2022
</p>
</li>
<li><p>Machine Learning Intern, <a href="https://www.sensetime.com/en" target=&ldquo;blank&rdquo;>SenseTime Research</a>, Oct. 2020 – Jul. 2021
</p>
</li>
<li><p>Machine Learning Intern, <a href="https://www.apple.com/" target=&ldquo;blank&rdquo;>Apple</a>, Mar. 2020 - Sep. 2020
</p>
</li>
<li><p>Research Intern, <a href="https://www.cmu.edu/" target=&ldquo;blank&rdquo;>Carnegie Mellon University</a>, Apr. 2019 - Mar. 2020
</p>
</li>
<li><p>Machine Learning Intern, <a href="http://research.lenovo.com/webapp/view_English/index.html" target=&ldquo;blank&rdquo;>Lenovo Research AI Lab</a>, Sep. 2019 - Dec. 2019
</p>
</li>
<li><p>Research Intern, <a href="https://www.tsinghua.edu.cn/en/" target=&ldquo;blank&rdquo;>Tsinghua University</a>, Apr. 2018 - Apr. 2019
</p>
</li>
</ul>
<h2>Projects</h2>

<p><section class="card">
</p>
<h2 id="b"><a href="DQ_CLIP.pdf" target=&ldquo;blank&rdquo;>DQ-CLIP: Post-Training Quantization for Dynamic Quantized CLIP.</a></h2>
<p><tt>#Multimodal, #Model Compression, #Large-Scale ML, #Quantization, #Scalable, #Efficiency</tt>

</p>
<table class="imgtable"><tr><td>
<img src="imgs/DQ_CLIP.png" alt="CLIP" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Although researchers have spent much effort on model compression to reduce the huge memory consumption and computational power of increasingly large models, how to compress multimodal models, especially Vision-Language Pretraining (VLP) models, is still under-explored. We know pruning and distillation can reduce the size of machine learning models, but typically require retraining with the same or even more computational resources. In contrast, post-training quantization incurs negligible costs. In this project, we aim to study the potential for reducing the size and computational complexity of a multimodel model (eg. CLIP) using post-training quantization without incurring significant additional computation. Our experiments showed that quantization has several advantages, including improved performance for devices with limited computational power. One of our key findings is that combined quantization of both the image and text modules leads to the best model size reduction, resulting in a 41% reduction in model size with only a moderate drop in accuracy. Our code is available at <a href="https://anonymous.4open.science/r/260d-project-22-fall" target=&ldquo;blank&rdquo;>here</a>.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="DQ_CLIP.pdf" target=&ldquo;blank&rdquo;>DQ-CLIP: Post-Training Quantization for Dynamic Quantized CLIP.</a></b>
<br />
Fall 2022, UCLA CS 260 project, advised by Prof. <a href="http://web.cs.ucla.edu/~baharan/index.htm" target=&ldquo;blank&rdquo;>Baharan Mirzasoleiman</a>, worked with Yadi Cao, Zhaoqian Wang.
</p>
</li>
</ul>
<p></section>
</p>

<p><section class="card">
</p>
<h2 id="a"><a href="Multimodal_Attack.pdf" target=&ldquo;blank&rdquo;>Probing the Need for Adversarial Attack on Multimodal Models.</a></h2>
<p><tt>#Multimodal, #Robustness, #Adversarial Attack, #Interpretability, #Trustworthy AI</tt>
<br />
<tt>#Adversarial Robustness of Machine Learning Models</tt>

</p>
<table class="imgtable"><tr><td>
<img src="imgs/attack.png" alt="Attack" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Our experience of the world is multimodal (vision, language, audio, etc). Although multimodal learning has recently revolutionized the state-of-the-art performance in many downstream tasks, the studies regarding its adversarial robustness remain largely unexplored. Therefore, in this project, we aim to provide an empirical study of adversarial attacks on some representative multimodal models, such as CLIP. Firstly, we compare how adversarial attacks affect the performance of multi-modal and uni-modal models. By analyzing the attack success rate (ASR) of adversarial attacks under different settings, we show the necessity of studying adversarial attacks on multimodal models. Then we conduct extensive experiments to discuss different aspects of the attack on uni-modal and multi-modal and found that the text model is often more robust than the image model. Last, we propose a simple yet effective iterative Co-Attack method, which achieved better performance than the original Co-Attack. We hope these key observations can provide guidance for both designing dedicated multimodal adversarial attacks and robust multimodal models, and inspire researchers better understand the vulnerabilities of multimodal models and develop effective methods for defending against adversarial attacks. Our code is available at: <a href="https://github.com/Eurus-Holmes/Multimodal-Attack" target=&ldquo;blank&rdquo;>https://github.com/Eurus-Holmes/Multimodal-Attack</a>.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="Multimodal_Attack.pdf" target=&ldquo;blank&rdquo;>Probing the Need for Adversarial Attack on Multimodal Models.</a></b>
<br />
Fall 2022, UCLA CS 269 project, advised by Prof. <a href="http://web.cs.ucla.edu/~chohsieh/" target=&ldquo;blank&rdquo;>Cho-Jui Hsieh</a>, worked with Siyan Dong, Xinyu Zhao, Yongqian Li.
</p>
</li>
</ul>
<p></section>
</p>

<p><section class="card">
</p>
<h2 id="1"><a href="debiasVL.pdf" target=&ldquo;blank&rdquo;>Measuring and Mitigating Bias in Vision-and-Language Models.</a></h2>
<p><tt>#Multimodal, #Vision-and-Language, #Fairness-and-Bias, #Interpretability, #Explainability</tt>
<br />
<tt>#Human-Centered AI for Computer Vision and Machine Autonomy</tt>

</p>
<table class="imgtable"><tr><td>
<img src="imgs/bias.png" alt="Bias" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Models pre-trained on large amounts of image-caption data have demonstrated impressive performance across vision-and-language (VL) tasks. However, societal biases have been serious issues in existing vision or language tasks and careful calibrations are required before deploying models in real-world settings, while only a few recent works have paid attention to the social bias problem in these models. In this work, we first propose a retrieval-based metric to measure gender and racial biases in two representative VL models (CLIP and FIBER). Then, we propose two post-training methods for debiasing VL models: subspace-level transformation and neuron-level manipulation. By identifying model output neurons or subspaces that correspond to the specific bias attributes, based on which we manipulate the model outputs to mitigate these biases. Extensive experimental results on the FairFace and COCO datasets demonstrate that our models can successfully reduce the societal bias in VL models while not hurting the model performance too much. We further perform analyses to show potential applications of our models on downstream tasks, including reversing gender neurons to revise images and mitigating the bias in text-driven image generation models.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="debiasVL.pdf" target=&ldquo;blank&rdquo;>Measuring and Mitigating Bias in Vision-and-Language Models.</a></b>
<br />
Spring 2022, <a href="https://github.com/UCLAdeepvision/CS269-projects-2022spring" target=&ldquo;blank&rdquo;>UCLA CS 269</a> project, advised by Prof. <a href="https://boleizhou.github.io/" target=&ldquo;blank&rdquo;>Bolei Zhou</a>, worked with Zi-Yi Dou.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="5"><a href="Sampling_for_Heterogeneous_Graph_Neural_Networks.pdf" target=&ldquo;blank&rdquo;>Sampling for Heterogeneous Graph Neural Networks.</a></h2>
<p><tt>#GNNs, #Graph Sampling, #Heterogeneous Graph, #Scalable, #Efficiency</tt>
<br />

</p>
<table class="imgtable"><tr><td>
<img src="imgs/HeteSampling.png" alt="HeteSampling" width="450px" />&nbsp;</td>
<td align="left"><p><i>Graph sampling is a popular technique in training large-scale graph neural networks (GNNs); recent sampling-based methods have demonstrated impressive success for homogeneous graphs. However, in practice, the interaction between different entities is often different based on their relationship, i.e., the network in reality is mostly heterogeneous. But only a few of the recent works have paid attention to sampling methods on heterogeneous graphs. In this work, we aim to study sampling for heterogeneous GNNs. We propose two general pipelines for heterogeneous sampling. Based on the proposed pipeline, we evaluate 3 representative sampling methods on heterogeneous graphs, including node-wise sampling, layer-wise sampling, and subgraph-wise sampling. To the best of our knowledge, we are the first to provide a thorough implementation, evaluation, and discussion of each sampling method on heterogeneous graphs. Extensive experiments compared sampling methods from multiple aspects and highlight their characteristics for each category. Evaluation of scalability on larger-scale heterogeneous graphs also shows we achieve the trade-off between efficiency and effectiveness. Last, we also analyze the limitations of our proposed pipeline on heterogeneous sub-graph sampling and provide a detailed comparison with HGSampling. Our code is available at: <a href="https://github.com/Eurus-Holmes/Heterogeneous_Sampling" target=&ldquo;blank&rdquo;>https://github.com/Eurus-Holmes/Heterogeneous_Sampling</a>.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="Sampling_for_Heterogeneous_Graph_Neural_Networks.pdf" target=&ldquo;blank&rdquo;>Sampling for Heterogeneous Graph Neural Networks.</a></b>
<br /> 
Spring 2022, <a href="https://github.com/yichousun/Spring2022_CS249_GNN" target=&ldquo;blank&rdquo;>UCLA CS 249</a> project, advised by Prof. <a href="https://web.cs.ucla.edu/~yzsun/" target=&ldquo;blank&rdquo;>Yizhou Sun</a>, worked with Yongqian Li, Ruoyu He, YuanChing Lin.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="8"><a href="Empirical_Study_of_Model_Compression_and_Speed_up_for_Vision_Transformer.pdf" target=&ldquo;blank&rdquo;>Empirical Study of Model Compression and Speed up for Vision Transformer.</a></h2>
<p><tt>#Vision Transformer, #Model Compression, #Fast Training, #Data-Efficient, #Large-Scale ML</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/ViT.png" alt="ViT" width="450px" />&nbsp;</td>
<td align="left"><p><i>Vision transformers (ViT) have recently attracted considerable attention and achieved SOTA performance in many computer vision tasks. However, ViT models suffer from excessive computational and memory costs due to stacking multi-head self-attention modules and else. To allow its further application on resource-restricted and low-powered devices, model compression techniques are required to reduce the model size as well as speed up inference with acceptable precision loss. In this work, we study four main types of model compression methods for ViT, including quantization, low-rank approximation, knowledge distillation, and pruning, and make a comprehensive comparative analysis. Furthermore, we also explore combinations of different compression methods to verify whether better performance can be obtained. Extensive experimental results show our methods achieve a decent trade-off between accuracy and computational efficiency.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="Empirical_Study_of_Model_Compression_and_Speed_up_for_Vision_Transformer.pdf" target=&ldquo;blank&rdquo;>Empirical Study of Model Compression and Speed up for Vision Transformer.</a></b>
<br /> 
Winter 2022, UCLA CS 260 project, advised by Prof. <a href="http://web.cs.ucla.edu/~chohsieh/" target=&ldquo;blank&rdquo;>Cho-Jui Hsieh</a>, worked with Tianyi Xie, Yongqian Li, Zhicheng Ren.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="10"><a href="https://chenfeiyang.top/DDL/" target=&ldquo;blank&rdquo;>DDL: Deep Deformable Learning for Image Segmentation.</a></h2>
<p><tt>#Computer Vision, #Deep Learning, #Deformable Models, #Geometry, #Image Segmentation</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/deformable.png" alt="deformable" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Image segmentation is a computer vision task of extracting pixel-wise mask segments of objects for an image. While modern Deep Learning-based models have achieved great success in various image segmentation tasks such as scene understanding, medical image analysis, applications for driver-less cars, and more, they can still be improved in terms of sheer performance, model explainability, and computational complexity. In this work, we study the effectiveness of deep learning-based models, deformable models, and finally the combination of the two over various Image Segmentation datasets. We aim to combine the classical geometry and physics-based approach of deformable models with modern large-scale advances in deep learning in order to propose a novel model architecture. We finally perform an empirical analysis of various models on a suite of datasets and metrics to do a comparative case study.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://chenfeiyang.top/DDL/" target=&ldquo;blank&rdquo;>DDL: Deep Deformable Learning for Image Segmentation.</a></b>
<br /> 
Fall 2021, UCLA CS 269 project, advised by Prof. <a href="http://web.cs.ucla.edu/~dt/" target=&ldquo;blank&rdquo;>Demetri Terzopoulos</a>, worked with Arjun Kallapur, Nischal Chandra, Sanjeev Venkatesan, Tianyi Xie, Vaibhav Kumar.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="2"><a href="https://github.com/Eurus-Holmes/Tumor2Graph" target=&ldquo;blank&rdquo;>Tumor2Graph: a novel Overall-Tumor-Profile-derived virtual graph deep learning for predicting tumor typing and subtyping.</a></h2>
<p><tt>#Multimodal, #GNNs, #Graph Embedding, #AI for Healthcare, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/Tumor2Graph.png" alt="Tumor2Graph" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>In this research, we propose a novel multimodal graph framework, namely Tumor2Graph, to jointly model 7 types of biomarkers (including structured data and unstructured data) for predicting tumor typing and subtyping. For the structured data (CNV, SNV (gene mutations), DNA methylation, mRNA (gene expression), miRNA, protein), We use element-wise add to integrate the primary feature embedding vectors in the first and second fully connected layers and the Laplacian smoothing embedding vectors in the graph convolutional layer. For the unstructured data (pathology images), we separate their feature extraction algorithms due to their specificity. We use a neural module including a 2D convectional layer and concatenate the extracted feature embedding vectors with those of the structured data to work as patient embedding vectors. The patient embedding vectors are directly used for supervised learning to classify tumor typing and unsupervised learning to cluster tumor subtyping.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://github.com/Eurus-Holmes/Tumor2Graph" target=&ldquo;blank&rdquo;>Tumor2Graph: a novel Overall-Tumor-Profile-derived virtual graph deep learning for predicting tumor typing and subtyping.</a></b>
<br /> 
SenseTime Internship project, Oct. 2020 – Jul. 2021.

</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="6"><a href="https://www.nature.com/articles/s42256-022-00553-w" target=&ldquo;blank&rdquo;>Predicting unseen antibodies' neutralizability via adaptive graph neural networks.</a></h2>
<p><tt>#GNNs, #Robustness, #Generalization, #AI for Healthcare, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/DeepAAI.png" alt="DeepAAI" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Most natural and synthetic antibodies are 'unseen'. That is, the demonstration of their neutralization effects with any antigen requires laborious and costly wet-lab experiments. The existing methods that learn antibody representations from known antibody–antigen interactions are unsuitable for unseen antibodies owing to the absence of interaction instances. The DeepAAI method proposed herein learns unseen antibody representations by constructing two adaptive relation graphs among antibodies and antigens and applying Laplacian smoothing between unseen and seen antibodies' representations. Rather than using static protein descriptors, DeepAAI learns representations and relation graphs 'dynamically', optimized towards the downstream tasks of neutralization prediction and 50% inhibition concentration estimation. The performance of DeepAAI is demonstrated on human immunodeficiency virus, severe acute respiratory syndrome coronavirus 2, influenza and dengue. Moreover, the relation graphs have rich interpretability. The antibody relation graph implies similarity in antibody neutralization reactions, and the antigen relation graph indicates the relation among a virus's different variants. We accordingly recommend probable broad-spectrum antibodies against new variants of these viruses.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://rdcu.be/cZngY" target=&ldquo;blank&rdquo;>Predicting unseen antibodies’ neutralizability via adaptive graph neural networks.</a></b>
<br /> 
SenseTime Internship project, Oct. 2020 – Jul. 2021. Published in <font color="red">Nature Machine Intelligence, 2022 (IF: 25.9)</font>.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="7"><a href="https://arxiv.org/abs/2101.12547" target=&ldquo;blank&rdquo;>BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein Interactions.</a></h2>
<p><tt>#GNNs, #Drug Discovery, #DPI, #AI for Healthcare, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/BridgeDPI.png" alt="BridgeDPI" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>In this research, we propose a hyper-nodes GNN for bridging drug-protein interactions and applied it to drug discovery, namely BridgeDPI. BridgeDPI introduces a class of nodes named hyper-nodes, which bridge different proteins drugs to work as PPAs and DDAs. The hyper-nodes can be supervised learned for the specific task of DPI since the whole process is end-to-end learning. Consequently, such a model would improve the prediction performance of DPI. In three real-world datasets, we further demonstrate that BridgeDPI outperforms state-of-the-art methods. Moreover, ablation studies verify the effectiveness of the hyper-nodes. Last, in an independent verification, BridgeDPI explores the candidate bindings among COVID-19’s proteins and various antiviral drugs. And the predictive results accord with the statement of the World Health Organization and Food and Drug Administration, showing the validity and reliability of BridgeDPI.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://arxiv.org/abs/2101.12547" target=&ldquo;blank&rdquo;>BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein Interactions.</a></b>
<br /> 
SenseTime Internship project, Oct. 2020 – Jul. 2021. Published in Bioinformatics, 2022.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="9"><a href="https://github.com/Eurus-Holmes/CHABCNet" target=&ldquo;blank&rdquo;>CHABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network</a></h2>
<p><tt>#Computer Vision, #Model Compression, #Auto ML, #Multilingual OCR, #Detectron2</tt>
<br />
<tt>#Data Augmentation, #Pre-training, #Robustness, #Generalization, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/ABCNet.png" alt="ABCNet" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>CHABCNet: Implemented an efficient Chinese scene end-to-end text spotting framework named CHABCNet, made key improvements (including data augmentation, auto ML, model distillation, etc.) based on the English ABCNet (CVPR 2020 Oral) and increased accuracy by 10% in terms of baseline. More importantly, CHABCNet is lightweight (reduced by 40%) and has a faster inference speed (10 times), which can be applied to practical scenarios. Also collaborated with a team of 3 to conduct ICDAR Robust Reading Challenge on Reading Chinese Text on Signboard (ReCTS). Building on Detectron2 (Facebook AI Research), achieved experimental performance equivalent to Top 3 and the speed was more than 10 times faster than baseline methods, met realistic needs.</i>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/SynthText.png" alt="SynthText.png" width="450px" height="150px" />&nbsp;</td>
<td align="left"><p><i>SynthText_CH: Generated 600k images including Chinese synthetic text, based on Synthetic Data for Text Localisation in Natural Images (CVPR 2016), improved model’s robustness and generalization ability. Sped up multi-GPU parallel training by splitting datasets to store on object storage service (Amazon S3) and pre-downloading onto SSD to save cost of storage by 60%.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://drive.google.com/file/d/1lmjzOucVY5NpuNsOSJ9URJIPFl-OitDl/view" target=&ldquo;blank&rdquo;>Project Presentation</a></b> | <b><a href="https://github.com/Eurus-Holmes/CHABCNet" target=&ldquo;blank&rdquo;>CHABCNet</a></b> | <b><a href="https://github.com/Eurus-Holmes/SynthText_CH" target=&ldquo;blank&rdquo;>SynthText_CH</a></b>
<br /> 
Apple Internship project (Under NDA), Mar. 2020 - Sep. 2020.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="3"><a href="https://github.com/Eurus-Holmes/MNMT" target=&ldquo;blank&rdquo;>Multimodal Neural Machine Translation</a></h2>
<p><tt>#Multimodal, #NLP, #Computer Vision, #Machine Translation, #Vision-and-Language</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/Multi30k.png" alt="Multi30k" width="450px" />&nbsp;</td>
<td align="left"><p><i>Multimodal Neural Machine Translation (MNMT) aims to combine information from other modality (such as images) to enhance text-only translation, has attracted wide attention from computer vision and natural language processing communities. For a long time, machine translation has only involved the conversion between texts, but in fact, human perception functions are multimodal. Because humans understand the world not only with text but also with visual and auditory perception capabilities. However, the semantics remains the same in the process of translation, because people have the same cognition of the nature of the objective world, but the &ldquo;perception method&rdquo; is different, which is reflected in the grammatical difference in language. Therefore, we can assume that incorporating such &ldquo;objective world knowledge&rdquo; (e.g. visual information in image/video-guided translation or audio information in spoken language translation) into the traditional text-only machine translation model can further improve the performance of machine translation, and solve the problem of data sparsity and ambiguity. In this work, we propose a novel representation learning method combined with visual contents and text information to improve the performance of MNMT, which can get dynamic hierarchical visual representation to make visual information better interact with language context. Compared with the experimental results of pure text machine translation on the Multi30k dataset, the BLEU value is increased by 3%.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://github.com/Eurus-Holmes/MNMT" target=&ldquo;blank&rdquo;>Multimodal Neural Machine Translation</a></b>
<br /> 
Lenovo Internship project, Sep. 2019 - Dec. 2019.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="11"><a href="https://www.mdpi.com/1999-4893/13/5/126/htm" target=&ldquo;blank&rdquo;>PUB-SalNet: A Pre-trained Unsupervised Self-Aware Backpropagation Network for Biomedical Salient Segmentation.</a></h2>
<p><tt>#Computer Vision, #Unsupervised Learning, #Pre-training, #Image Segmentation, #Object Detection</tt>
<br />
<tt>#Saliency, #Data Augmentation, #Robustness, #Generalization, #AI for Healthcare</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/PUB-SalNet.png" alt="PUB-SalNet" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>In this paper, we propose a completely unsupervised self-aware network based on pre-training and attentional backpropagation for biomedical salient segmentation, named as PUB-SalNet. Firstly, we aggregate a new biomedical data set from several simulated Cellular Electron Cryo-Tomography (CECT) data sets featuring rich salient objects, different SNR settings, and various resolutions, which is called SalSeg-CECT. Based on the SalSeg-CECT data set, we then pre-train a model specially designed for biomedical tasks as a backbone module to initialize network parameters. Next, we present a U-SalNet network to learn to selectively attend to salient objects. It includes two types of attention modules to facilitate learning saliency through global contrast and local similarity. Lastly, we jointly refine the salient regions together with feature representations from U-SalNet, with the parameters updated by self-aware attentional backpropagation. We apply PUB-SalNet for analysis of 2D simulated and real images and achieve state-of-the-art performance on simulated biomedical data sets. Furthermore, our proposed PUB-SalNet can be easily extended to 3D images. The experimental results on the 2d and 3d data sets also demonstrate the generalization ability and robustness of our method.</i>
</p>
</td></tr></table>
<h3>Papers</h3>
<ul>
<li><p><b><a href="https://www.mdpi.com/1999-4893/13/5/126/htm" target=&ldquo;blank&rdquo;>PUB-SalNet: A Pre-trained Unsupervised Self-Aware Backpropagation Network for Biomedical Salient Segmentation.</a></b>
<br /> 
<b>Feiyang Chen*</b>, Ying Jiang*, Xiangrui Zeng and Min Xu. (*equal contribution). Algorithms 2020, 13(5), 126.
</p>
</li>
<li><p><b><a href="Is_Deep_Learning_All_You_Need_for_Unsupervised_Saliency_Detection_of_Biomedical_Images_.pdf" target=&ldquo;blank&rdquo;>Is Deep Learning All You Need for Unsupervised Saliency Detection of Biomedical Images?</a></b>
<br /> 
<b>Feiyang Chen</b>, Ying Jiang, Xiangrui Zeng, and Min Xu. (Manuscript), 2019.
</p>
</li>
<li><p><b><a href="https://pubmed.ncbi.nlm.nih.gov/32907544/" target=&ldquo;blank&rdquo;>A unified framework for packing deformable and non-deformable subcellular structures in crowded cryo-electron tomogram simulation.</a></b>
<br /> 
Sinuo Liu, Xiaojuan Ban, Xiangrui Zeng, Fengnian Zhao, Yuan Gao, Wenjie Wu, Hongpan Zhang, <b>Feiyang Chen</b>, Thomas Hall, Xin Gao and Min Xu. BMC Bioinformatics 2020.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="4"><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment Analysis.</a></h2>
<p><tt>#Multimodal, #NLP, #Speech and Audio, #Sentiment Analysis, #Feature Fusion</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/MSA.png" alt="MSA" width="300px" />&nbsp;</td>
<td align="left"><p><i>Sentiment analysis, mostly based on text, has been rapidly developing in the last decade and has attracted widespread attention in both academia and industry. However, the information in the real world usually comes from multiple modalities, such as audio and text. Therefore, in this paper, based on audio and text, we consider the task of multimodal sentiment analysis and propose a novel fusion strategy including both multi-feature fusion and multi-modality fusion to improve the accuracy of audio-text sentiment analysis. We call it the DFF-ATMF (Deep Feature Fusion - Audio and Text Modality Fusion) model, which consists of two parallel branches, the audio modality based branch and the text modality based branch. Its core mechanisms are the fusion of multiple feature vectors and multiple modality attention. Experiments on the CMU-MOSI dataset and the recently released CMU-MOSEI dataset, both collected from YouTube for sentiment analysis, show the very competitive results of our DFF-ATMF model. Furthermore, by virtue of attention weight distribution heatmaps, we also demonstrate the deep features learned by using DFF-ATMF are complementary to each other and robust. Surprisingly, DFF-ATMF also achieves new state-of-the-art results on the IEMOCAP dataset, indicating that the proposed fusion strategy also has a good generalization ability for multimodal emotion recognition.</i>
</p>
</td></tr></table>
<h3>Papers</h3>
<ul>
<li><p><b><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment Analysis.</a></b> 
<br /> 
<b>Feiyang Chen</b>, Ziqian Luo, Yanyan Xu and Dengfeng Ke. In Proceedings of the AAAI-20 Workshop on Affective Content Analysis, New York, USA, AAAI.
</p>
</li>
<li><p><b><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Audio-Text Sentiment Analysis using Deep Robust Complementary Fusion of Multi-Features and Multi-Modalities.</a></b> 
<br /> 
<b>Feiyang Chen</b>, Ziqian Luo. Accepted by NeurIPS 2019 Workshop: NewInML 2019, Vancouver, BC, Canada, NeurIPS.
</p>
</li>
<li><p><b><a href="http://ceur-ws.org/Vol-2328/3_2_paper_17.pdf" target=&ldquo;blank&rdquo;>Audio Sentiment Analysis by Heterogeneous Signal Features Learned from Utterance-Based Parallel Neural Network.</a></b> 
<br /> 
Ziqian Luo, Hua Xu and <b>Feiyang Chen</b>. In Proceedings of the AAAI-19 Workshop on Affective Content Analysis, Honolulu, USA, AAAI.
</p>
</li>
</ul>
<p></section>
</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-12-09, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
<!-- hitwebcounter Code START -->
<a href="https://www.hitwebcounter.com" target="_blank">
<img src="https://hitwebcounter.com/counter/counter.php?page=7990466&style=0027&nbdigits=5&type=page&initCount=10000" title="Free Counter" Alt="web counter"   border="0" /></a>              
</div>
</div>
</div>
</body>
</html>
