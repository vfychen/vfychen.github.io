<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Feiyang(Vance) Chen</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3FPKB92VCF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3FPKB92VCF');
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-471P957C5W"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-471P957C5W');
</script>

</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Feiyang(Vance) Chen</h1>
</div>
<table class="imgtable"><tr><td>
<img src="vfychen.jpg" alt="Feiyang(Vance) Chen" />&nbsp;</td>
<td align="left"><p><a href="https://web.cs.ucla.edu/~fychen/" target=&ldquo;blank&rdquo;>Feiyang(Vance) Chen</a> <br />

Graduate Student, <a href="https://www.cs.ucla.edu/" target=&ldquo;blank&rdquo;>Department of Computer Science</a> <br />
<a href="https://www.ucla.edu/" target=&ldquo;blank&rdquo;>University of California, Los Angeles</a> <br />

Researcher at <a href="https://www.nightai.co/" target=&ldquo;blank&rdquo;>Night AI</a>, <i><a href="https://twitter.com/thenightaico" style="color:red;">drop us a line to join!</a></i> <br /> 
</p>
<p>Email: fychen [at] cs [dot] ucla [dot] edu <br />
<a href="https://github.com/Eurus-Holmes" target=&ldquo;blank&rdquo;>GitHub</a> | <a href="https://twitter.com/vfychen" target=&ldquo;blank&rdquo;>Twitter</a> | <a href="https://scholar.google.com/citations?hl=en&amp;user=yJgHVyYAAAAJ&amp;view_op=list_works&amp;authuser=4&amp;gmla=AJsN-F4z0Bs5_LJ279QpRClNPQwEs1siVB68lbpziOblP1SJnJayn2mqwfHzej4BpqGxd0dwf3YJHvQRbjS-kNf31iEqB6FOqFySuFKH20lCWz2ox2CQkew" target=&ldquo;blank&rdquo;>Google Scholar</a> | <a href="CV.pdf" target=&ldquo;blank&rdquo;>Curriculum Vitae</a> <br />
</p>
<p><i><font color="red">Actively seeking full-time opportunities starting 2023!</font></i> <br />
<i><font color="red">Always open to collaborations on Multimodal and Interdisciplinary research topics!</font></i>
</p>
</td></tr></table>
<h2>About me</h2>
<p>I'm a final-year CS graduate student at <a href="http://www.ucla.edu/" target=&ldquo;blank&rdquo;>UCLA</a>, my academic advisor is Prof. <a href="http://web.cs.ucla.edu/~dt/">Demetri Terzopoulos</a>. My current research interest focuses on <b>Multimodal Machine Learning</b> and <b>AI for Good</b>. As the nature of human communication is <b>Multimodal</b>, I aspire to get computers to reach new heights in their understanding and applied <b>AI for Good</b> social impacts.<br />
</p>
<p>In my previous experiences, I have a diverse academic and industrial background, including Multimodal AI <sup><a href="https://github.com/Eurus-Holmes/Awesome-Multimodal-Research">[0]</a>,<a href="#1">[1]</a>,<a href="#2">[2]</a>,<a href="#3">[3]</a>,<a href="#4">[4]</a></sup>, Graph Neural Networks (GNNs) <sup><a href="#5">[5]</a>,<a href="#2">[2]</a>,<a href="#6">[6]</a>,<a href="#7">[7]</a></sup>, Model Compression &amp; Efficient ML <sup><a href="#5">[5]</a>,<a href="#8">[8]</a>,<a href="#9">[9]</a></sup>, Computer Vision <sup><a href="#9">[9]</a>,<a href="#10">[10]</a>,<a href="#11">[11]</a></sup>, NLP <sup><a href="#1">[1]</a>,<a href="#3">[3]</a>,<a href="#4">[4]</a></sup>, AI for Healthcare <sup><a href="#2">[2]</a>,<a href="#6">[6]</a>,<a href="#7">[7]</a></sup>, Data Augmentation <sup><a href="#9">[9]</a>,<a href="#11">[11]</a></sup>, Large-Scale ML <sup><a href="#5">[5]</a>,<a href="#8">[8]</a></sup>, Auto ML <sup><a href="#9">[9]</a></sup>, Speech &amp; Audio <sup><a href="#4">[4]</a></sup>, and solid software engineering experiences in algorithm design, data structures, problem-solving, and complexity. I’m also active in academics and have served as a reviewer for many top-tier AI/ML conferences, like ICLR, ICML, NeurIPS, etc. </p>
<p>I love <b>open-source</b> community and making contributions to the open-source world, like <a href="https://github.com/Eurus-Holmes" target=&ldquo;blank&rdquo;>GitHub</a>. In <a href="https://gitstar-ranking.com/" target=&ldquo;blank&rdquo;>Gitstar Ranking</a>, I rank <a href="https://gitstar-ranking.com/Eurus-Holmes" target=&ldquo;blank&rdquo;>Top 0.03%</a> of all GitHub developers (more than 37,000,000) by GitHub stars. In <a href="https://codersrank.io/">CodersRank</a>, I rank <a href="https://profile.codersrank.io/user/eurus-holmes">Top 3%</a> of all 62k developers worldwide. <a href="https://github-profile-trophy.vercel.app/?username=Eurus-Holmes&amp;column=7&amp;theme=dracula&amp;rank=SECRET,SSS,SS,S" target=&ldquo;blank&rdquo;>Here</a> is my <a href="https://github.com/ryo-ma/github-profile-trophy" target=&ldquo;blank&rdquo;>Github Profile Trophy</a>, I have 11 S (<b>Super Stargazer</b>) GitHub Star Trophies. I'd like to discover, share, and build better software for this world. I am the core member of some open-source organizations, such as <a href="https://github.com/TheAlgorithms" target=&ldquo;blank&rdquo;>The Algorithms</a>, <a href="https://github.com/apachecn" target=&ldquo;blank&rdquo;>ApacheCN</a>, and <a href="https://github.com/doocs" target=&ldquo;blank&rdquo;>Doocs</a>. I am also a student member at <a href="https://www.ieee.org/" target=&ldquo;blank&rdquo;>IEEE</a> and <a href="https://www.acm.org/" target=&ldquo;blank&rdquo;>ACM</a>.
</p>
<p>I have been pondering how machines can break out of their computational boundaries to understand human intelligence. My goal is to develop computationally efficient machine learning as well as deep learning models and algorithms, building the computational foundations to enable computers with the abilities to analyze, recognize and predict subtle human communicative behaviors during social interactions. My favorite quote: 
<u><i>&ldquo;The thing that’s worth doing is trying to improve our understanding of the world and gain a better appreciation of the universe and not to worry too much about there being no meaning.&rdquo;</i></u>
</p>
<h2>Research Interests </h2>
<ul>
<li><p><b>Multimodal Machine Learning</b>: representation, alignment, translation, fusion, and co-learning of heterogeneous data
</p>
</li>
<li><p><b>Applied Machine Learning</b>: real-world applications in language, vision, speech, robotics, education, and self-driving
</p>
</li>
<li><p><b>AI + X</b>: AI for healthcare, finance, climate, agriculture, astronomy, art, and VR (Metaverse)
</p>
</li>
</ul>
<p><i><font color="red">Feel free to contact me if you’d like to talk about any of the above research topics, I'm always open to collaborations!</font></i>
</p>
<h2>Services</h2>
<ul>
<li><p>PC Member/Reviewer: ICLR 2023/2022/2021, ICML 2022, NeurIPS 2022/2021, ACL 2022/2021/2020, EMNLP 2022/2021/2020, <br /> NAACL-HLT 2021, EACL 2021, AACL-IJCNLP 2022/2020, WACV 2020, ICMI 2021/2020/2019
</p>
</li>
<li><p>Founder of open-source organizations: <a href="https://github.com/FGDBTKD" target=&ldquo;blank&rdquo;>FGDBTKD</a>, <a href="https://github.com/multimodal-machine-learning" target=&ldquo;blank&rdquo;>MMLG</a>, <a href="https://github.com/neural-machine-translation" target=&ldquo;blank&rdquo;>NMTG</a>
</p>
</li>
<li><p>Core member of open-source organizations: <a href="https://github.com/TheAlgorithms" target=&ldquo;blank&rdquo;>The Algorithms</a>, <a href="https://github.com/apachecn" target=&ldquo;blank&rdquo;>ApacheCN</a>, <a href="https://github.com/doocs" target=&ldquo;blank&rdquo;>Doocs</a>
</p>
</li>
</ul>
<h2>Experiences</h2>
<ul>
<li><p>Visiting Student Researcher, <a href="https://www.salesforceairesearch.com/" target=&ldquo;blank&rdquo;>Salesforce Research</a>, 2022
</p>
</li>
<li><p>Machine Learning Intern, <a href="https://www.aboutcoupang.com/" target=&ldquo;blank&rdquo;>Coupang</a>, Jun. 2022 – Sep. 2022
</p>
</li>
<li><p>Machine Learning Intern, <a href="https://www.sensetime.com/en" target=&ldquo;blank&rdquo;>SenseTime Research</a>, Oct. 2020 – Jul. 2021
</p>
</li>
<li><p>Machine Learning Intern, <a href="https://www.apple.com/" target=&ldquo;blank&rdquo;>Apple</a>, Mar. 2020 - Sep. 2020
</p>
</li>
<li><p>Research Intern, <a href="https://www.cmu.edu/" target=&ldquo;blank&rdquo;>Carnegie Mellon University</a>, Apr. 2019 - Mar. 2020
</p>
</li>
<li><p>Machine Learning Intern, <a href="http://research.lenovo.com/webapp/view_English/index.html" target=&ldquo;blank&rdquo;>Lenovo Research AI Lab</a>, Sep. 2019 - Dec. 2019
</p>
</li>
<li><p>Research Intern, <a href="https://www.tsinghua.edu.cn/en/" target=&ldquo;blank&rdquo;>Tsinghua University</a>, Apr. 2018 - Apr. 2019
</p>
</li>
</ul>
<h2>Projects &amp; Publications</h2>
<p><section class="card">
</p>
<h2 id="1"><a href="debiasVL.pdf" target=&ldquo;blank&rdquo;>Measuring and Mitigating Bias in Vision-and-Language Models.</a></h2>
<p><tt>#Multimodal, #Vision-and-Language, #Fairness-and-Bias, #Interpretability, #Explainability</tt>
<br />
<tt>#Human-Centered AI for Computer Vision and Machine Autonomy</tt>

</p>
<table class="imgtable"><tr><td>
<img src="imgs/bias.png" alt="Bias" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Models pre-trained on large amounts of image-caption data have demonstrated impressive performance across vision-and-language (VL) tasks. However, societal biases have been serious issues in existing vision or language tasks and careful calibrations are required before deploying models in real-world settings, while only a few recent works have paid attention to the social bias problem in these models. In this work, we first propose a retrieval-based metric to measure gender and racial biases in two representative VL models (CLIP and FIBER). Then, we propose two post-training methods for debiasing VL models: subspace-level transformation and neuron-level manipulation. By identifying model output neurons or subspaces that correspond to the specific bias attributes, based on which we manipulate the model outputs to mitigate these biases. Extensive experimental results on the FairFace and COCO datasets demonstrate that our models can successfully reduce the societal bias in VL models while not hurting the model performance too much. We further perform analyses to show potential applications of our models on downstream tasks, including reversing gender neurons to revise images and mitigating the bias in text-driven image generation models.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="debiasVL.pdf" target=&ldquo;blank&rdquo;>Measuring and Mitigating Bias in Vision-and-Language Models.</a></b>
<br />
Spring 2022, <a href="https://github.com/UCLAdeepvision/CS269-projects-2022spring" target=&ldquo;blank&rdquo;>UCLA CS 269</a> project, advised by Prof. <a href="https://boleizhou.github.io/" target=&ldquo;blank&rdquo;>Bolei Zhou</a>, worked with Zi-Yi Dou.
</p>
</li>
</ul>
<p></section>
</p>

<p><section class="card">
</p>
<h2 id="5"><a href="Sampling_for_Heterogeneous_Graph_Neural_Networks.pdf" target=&ldquo;blank&rdquo;>Sampling for Heterogeneous Graph Neural Networks.</a></h2>
<p><tt>#GNNs, #Graph Sampling, #Heterogeneous Graph, #Scalable, #Efficiency</tt>
<br />

</p>
<table class="imgtable"><tr><td>
<img src="imgs/HeteSampling.png" alt="HeteSampling" width="450px" />&nbsp;</td>
<td align="left"><p><i>Graph sampling is a popular technique in training large-scale graph neural networks (GNNs); recent sampling-based methods have demonstrated impressive success for homogeneous graphs. However, in practice, the interaction between different entities is often different based on their relationship, i.e., the network in reality is mostly heterogeneous. But only a few of the recent works have paid attention to sampling methods on heterogeneous graphs. In this work, we aim to study sampling for heterogeneous GNNs. We propose two general pipelines for heterogeneous sampling. Based on the proposed pipeline, we evaluate 3 representative sampling methods on heterogeneous graphs, including node-wise sampling, layer-wise sampling, and subgraph-wise sampling. To the best of our knowledge, we are the first to provide a thorough implementation, evaluation, and discussion of each sampling method on heterogeneous graphs. Extensive experiments compared sampling methods from multiple aspects and highlight their characteristics for each category. Evaluation of scalability on larger-scale heterogeneous graphs also shows we achieve the trade-off between efficiency and effectiveness. Last, we also analyze the limitations of our proposed pipeline on heterogeneous sub-graph sampling and provide a detailed comparison with HGSampling. Our code is available at: <a href="https://github.com/Eurus-Holmes/Heterogeneous_Sampling" target=&ldquo;blank&rdquo;>https://github.com/Eurus-Holmes/Heterogeneous_Sampling</a>.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="Sampling_for_Heterogeneous_Graph_Neural_Networks.pdf" target=&ldquo;blank&rdquo;>Sampling for Heterogeneous Graph Neural Networks.</a></b>
<br /> 
Spring 2022, <a href="https://github.com/yichousun/Spring2022_CS249_GNN" target=&ldquo;blank&rdquo;>UCLA CS 249</a> project, advised by Prof. <a href="https://web.cs.ucla.edu/~yzsun/" target=&ldquo;blank&rdquo;>Yizhou Sun</a>, worked with Yongqian Li, Ruoyu He, YuanChing Lin.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="8"><a href="Empirical_Study_of_Model_Compression_and_Speed_up_for_Vision_Transformer.pdf" target=&ldquo;blank&rdquo;>Empirical Study of Model Compression and Speed up for Vision Transformer.</a></h2>
<p><tt>#Vision Transformer, #Model Compression, #Fast Training, #Data-Efficient, #Large-Scale ML</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/ViT.png" alt="ViT" width="450px" />&nbsp;</td>
<td align="left"><p><i>Vision transformers (ViT) have recently attracted considerable attention and achieved SOTA performance in many computer vision tasks. However, ViT models suffer from excessive computational and memory costs due to stacking multi-head self-attention modules and else. To allow its further application on resource-restricted and low-powered devices, model compression techniques are required to reduce the model size as well as speed up inference with acceptable precision loss. In this work, we study four main types of model compression methods for ViT, including quantization, low-rank approximation, knowledge distillation, and pruning, and make a comprehensive comparative analysis. Furthermore, we also explore combinations of different compression methods to verify whether better performance can be obtained. Extensive experimental results show our methods achieve a decent trade-off between accuracy and computational efficiency.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="Empirical_Study_of_Model_Compression_and_Speed_up_for_Vision_Transformer.pdf" target=&ldquo;blank&rdquo;>Empirical Study of Model Compression and Speed up for Vision Transformer.</a></b>
<br /> 
Winter 2022, UCLA CS 260 project, advised by Prof. <a href="http://web.cs.ucla.edu/~chohsieh/" target=&ldquo;blank&rdquo;>Cho-Jui Hsieh</a>, worked with Tianyi Xie, Yongqian Li, Zhicheng Ren.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="10"><a href="https://chenfeiyang.top/DDL/" target=&ldquo;blank&rdquo;>DDL: Deep Deformable Learning for Image Segmentation.</a></h2>
<p><tt>#Computer Vision, #Deep Learning, #Deformable Models, #Geometry, #Image Segmentation</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/deformable.png" alt="deformable" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Image segmentation is a computer vision task of extracting pixel-wise mask segments of objects for an image. While modern Deep Learning-based models have achieved great success in various image segmentation tasks such as scene understanding, medical image analysis, applications for driver-less cars, and more, they can still be improved in terms of sheer performance, model explainability, and computational complexity. In this work, we study the effectiveness of deep learning-based models, deformable models, and finally the combination of the two over various Image Segmentation datasets. We aim to combine the classical geometry and physics-based approach of deformable models with modern large-scale advances in deep learning in order to propose a novel model architecture. We finally perform an empirical analysis of various models on a suite of datasets and metrics to do a comparative case study.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://chenfeiyang.top/DDL/" target=&ldquo;blank&rdquo;>DDL: Deep Deformable Learning for Image Segmentation.</a></b>
<br /> 
Fall 2021, UCLA CS 269 project, advised by Prof. <a href="http://web.cs.ucla.edu/~dt/" target=&ldquo;blank&rdquo;>Demetri Terzopoulos</a>, worked with Arjun Kallapur, Nischal Chandra, Sanjeev Venkatesan, Tianyi Xie, Vaibhav Kumar.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="2"><a href="https://github.com/Eurus-Holmes/Tumor2Graph" target=&ldquo;blank&rdquo;>Tumor2Graph: a novel Overall-Tumor-Profile-derived virtual graph deep learning for predicting tumor typing and subtyping.</a></h2>
<p><tt>#Multimodal, #GNNs, #Graph Embedding, #AI for Healthcare, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/Tumor2Graph.png" alt="Tumor2Graph" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>In this research, we propose a novel multimodal graph framework, namely Tumor2Graph, to jointly model 7 types of biomarkers (including structured data and unstructured data) for predicting tumor typing and subtyping. For the structured data (CNV, SNV (gene mutations), DNA methylation, mRNA (gene expression), miRNA, protein), We use element-wise add to integrate the primary feature embedding vectors in the first and second fully connected layers and the Laplacian smoothing embedding vectors in the graph convolutional layer. For the unstructured data (pathology images), we separate their feature extraction algorithms due to their specificity. We use a neural module including a 2D convectional layer and concatenate the extracted feature embedding vectors with those of the structured data to work as patient embedding vectors. The patient embedding vectors are directly used for supervised learning to classify tumor typing and unsupervised learning to cluster tumor subtyping.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://github.com/Eurus-Holmes/Tumor2Graph" target=&ldquo;blank&rdquo;>Tumor2Graph: a novel Overall-Tumor-Profile-derived virtual graph deep learning for predicting tumor typing and subtyping.</a></b>
<br /> 
SenseTime Internship project, Oct. 2020 – Jul. 2021.

</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="6"><a href="https://aai-test.github.io/" target=&ldquo;blank&rdquo;>Predicting Antigen-Antibody Interaction via Global and Local Feature Deep Learning based on Primary Amino Acid Sequences.</a></h2>
<p><tt>#GNNs, #Robustness, #Generalization, #AI for Healthcare, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/DeepAAI.png" alt="DeepAAI" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>Antibodies work as potent agents to opsonize and neutralize viruses. Antibodies’ rich variability protects hosts from miscellaneous intruders but poses a challenge for methods of identifying antigen-antibody interactions, especially on new (or unseen) antibodies due to their unknown interactability with antigens. Wet-lab experimental processes (such as phage display) are resource-intensive and time-consuming. Hence, we propose DeepAAI, a deep neural network-based tool. We devise an automatically learned virtual graph to address antibodies’ high variability, which connects seen and unseen antibodies by quantitating functional similarity based on the supervised signals from two downstream tasks, binary neutralization prediction and IC50 estimation. Substantial experiments on our curated dataset, including 29,394 antigen-antibody interactions of human immunodeficiency virus (HIV), prove DeepAAI’s accuracy and robustness. Moreover, we demonstrate DeepAAI’s generalizability on clinical samples, 17 neutralizing and 19 non-neutralizing antibodies of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), obtained from our previous study. Hopefully, DeepAAI can provide a rapid and precise approach to assist in laboratory experiments for discovering new antibodies.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://aai-test.github.io/" target=&ldquo;blank&rdquo;>Predicting Antigen-Antibody Interaction via Global and Local Feature Deep Learning based on Primary Amino Acid Sequences.</a></b>
<br /> 
SenseTime Internship project, Oct. 2020 – Jul. 2021.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="7"><a href="https://arxiv.org/abs/2101.12547" target=&ldquo;blank&rdquo;>BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein Interactions.</a></h2>
<p><tt>#GNNs, #Drug Discovery, #DPI, #AI for Healthcare, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/BridgeDPI.png" alt="BridgeDPI" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>In this research, we propose a hyper-nodes GNN for bridging drug-protein interactions and applied it to drug discovery, namely BridgeDPI. BridgeDPI introduces a class of nodes named hyper-nodes, which bridge different proteins drugs to work as PPAs and DDAs. The hyper-nodes can be supervised learned for the specific task of DPI since the whole process is end-to-end learning. Consequently, such a model would improve the prediction performance of DPI. In three real-world datasets, we further demonstrate that BridgeDPI outperforms state-of-the-art methods. Moreover, ablation studies verify the effectiveness of the hyper-nodes. Last, in an independent verification, BridgeDPI explores the candidate bindings among COVID-19’s proteins and various antiviral drugs. And the predictive results accord with the statement of the World Health Organization and Food and Drug Administration, showing the validity and reliability of BridgeDPI.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://arxiv.org/abs/2101.12547" target=&ldquo;blank&rdquo;>BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein Interactions.</a></b>
<br /> 
SenseTime Internship project, Oct. 2020 – Jul. 2021.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="9"><a href="https://github.com/Eurus-Holmes/CHABCNet" target=&ldquo;blank&rdquo;>CHABCNet: Real-time Scene Text Spotting with Adaptive Bezier-Curve Network</a></h2>
<p><tt>#Computer Vision, #Model Compression, #Auto ML, #Multilingual OCR, #Detectron2</tt>
<br />
<tt>#Data Augmentation, #Pre-training, #Robustness, #Generalization, #Applied Machine Learning</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/ABCNet.png" alt="ABCNet" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>CHABCNet: Implemented an efficient Chinese scene end-to-end text spotting framework named CHABCNet, made key improvements (including data augmentation, auto ML, model distillation, etc.) based on the English ABCNet (CVPR 2020 Oral) and increased accuracy by 10% in terms of baseline. More importantly, CHABCNet is lightweight (reduced by 40%) and has a faster inference speed (10 times), which can be applied to practical scenarios. Also collaborated with a team of 3 to conduct ICDAR Robust Reading Challenge on Reading Chinese Text on Signboard (ReCTS). Building on Detectron2 (Facebook AI Research), achieved experimental performance equivalent to Top 3 and the speed was more than 10 times faster than baseline methods, met realistic needs.</i>
</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<img src="imgs/SynthText.png" alt="SynthText.png" width="450px" height="150px" />&nbsp;</td>
<td align="left"><p><i>SynthText_CH: Generated 600k images including Chinese synthetic text, based on Synthetic Data for Text Localisation in Natural Images (CVPR 2016), improved model’s robustness and generalization ability. Sped up multi-GPU parallel training by splitting datasets to store on object storage service (Amazon S3) and pre-downloading onto SSD to save cost of storage by 60%.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://drive.google.com/file/d/1lmjzOucVY5NpuNsOSJ9URJIPFl-OitDl/view" target=&ldquo;blank&rdquo;>Project Presentation</a></b> | <b><a href="https://github.com/Eurus-Holmes/CHABCNet" target=&ldquo;blank&rdquo;>CHABCNet</a></b> | <b><a href="https://github.com/Eurus-Holmes/SynthText_CH" target=&ldquo;blank&rdquo;>SynthText_CH</a></b>
<br /> 
Apple Internship project (Under NDA), Mar. 2020 - Sep. 2020.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="3"><a href="https://github.com/Eurus-Holmes/MNMT" target=&ldquo;blank&rdquo;>Multimodal Neural Machine Translation</a></h2>
<p><tt>#Multimodal, #NLP, #Computer Vision, #Machine Translation, #Vision-and-Language</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/Multi30k.png" alt="Multi30k" width="450px" />&nbsp;</td>
<td align="left"><p><i>Multimodal Neural Machine Translation (MNMT) aims to combine information from other modality (such as images) to enhance text-only translation, has attracted wide attention from computer vision and natural language processing communities. For a long time, machine translation has only involved the conversion between texts, but in fact, human perception functions are multimodal. Because humans understand the world not only with text but also with visual and auditory perception capabilities. However, the semantics remains the same in the process of translation, because people have the same cognition of the nature of the objective world, but the &ldquo;perception method&rdquo; is different, which is reflected in the grammatical difference in language. Therefore, we can assume that incorporating such &ldquo;objective world knowledge&rdquo; (e.g. visual information in image/video-guided translation or audio information in spoken language translation) into the traditional text-only machine translation model can further improve the performance of machine translation, and solve the problem of data sparsity and ambiguity. In this work, we propose a novel representation learning method combined with visual contents and text information to improve the performance of MNMT, which can get dynamic hierarchical visual representation to make visual information better interact with language context. Compared with the experimental results of pure text machine translation on the Multi30k dataset, the BLEU value is increased by 3%.</i>
</p>
</td></tr></table>
<ul>
<li><p><b><a href="https://github.com/Eurus-Holmes/MNMT" target=&ldquo;blank&rdquo;>Multimodal Neural Machine Translation</a></b>
<br /> 
Lenovo Internship project, Sep. 2019 - Dec. 2019.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="11"><a href="https://www.mdpi.com/1999-4893/13/5/126/htm" target=&ldquo;blank&rdquo;>PUB-SalNet: A Pre-trained Unsupervised Self-Aware Backpropagation Network for Biomedical Salient Segmentation.</a></h2>
<p><tt>#Computer Vision, #Unsupervised Learning, #Pre-training, #Image Segmentation, #Object Detection</tt>
<br />
<tt>#Saliency, #Data Augmentation, #Robustness, #Generalization, #AI for Healthcare</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/PUB-SalNet.png" alt="PUB-SalNet" width="450px" height="250px" />&nbsp;</td>
<td align="left"><p><i>In this paper, we propose a completely unsupervised self-aware network based on pre-training and attentional backpropagation for biomedical salient segmentation, named as PUB-SalNet. Firstly, we aggregate a new biomedical data set from several simulated Cellular Electron Cryo-Tomography (CECT) data sets featuring rich salient objects, different SNR settings, and various resolutions, which is called SalSeg-CECT. Based on the SalSeg-CECT data set, we then pre-train a model specially designed for biomedical tasks as a backbone module to initialize network parameters. Next, we present a U-SalNet network to learn to selectively attend to salient objects. It includes two types of attention modules to facilitate learning saliency through global contrast and local similarity. Lastly, we jointly refine the salient regions together with feature representations from U-SalNet, with the parameters updated by self-aware attentional backpropagation. We apply PUB-SalNet for analysis of 2D simulated and real images and achieve state-of-the-art performance on simulated biomedical data sets. Furthermore, our proposed PUB-SalNet can be easily extended to 3D images. The experimental results on the 2d and 3d data sets also demonstrate the generalization ability and robustness of our method.</i>
</p>
</td></tr></table>
<h3>Papers</h3>
<ul>
<li><p><b><a href="https://www.mdpi.com/1999-4893/13/5/126/htm" target=&ldquo;blank&rdquo;>PUB-SalNet: A Pre-trained Unsupervised Self-Aware Backpropagation Network for Biomedical Salient Segmentation.</a></b>
<br /> 
<b>Feiyang Chen*</b>, Ying Jiang*, Xiangrui Zeng and Min Xu. (*equal contribution). Algorithms 2020, 13(5), 126.
</p>
</li>
<li><p><b><a href="Is_Deep_Learning_All_You_Need_for_Unsupervised_Saliency_Detection_of_Biomedical_Images_.pdf" target=&ldquo;blank&rdquo;>Is Deep Learning All You Need for Unsupervised Saliency Detection of Biomedical Images?</a></b>
<br /> 
<b>Feiyang Chen</b>, Ying Jiang, Xiangrui Zeng, and Min Xu. (Manuscript), 2019.
</p>
</li>
<li><p><b><a href="https://pubmed.ncbi.nlm.nih.gov/32907544/" target=&ldquo;blank&rdquo;>A unified framework for packing deformable and non-deformable subcellular structures in crowded cryo-electron tomogram simulation.</a></b>
<br /> 
Sinuo Liu, Xiaojuan Ban, Xiangrui Zeng, Fengnian Zhao, Yuan Gao, Wenjie Wu, Hongpan Zhang, <b>Feiyang Chen</b>, Thomas Hall, Xin Gao and Min Xu. BMC Bioinformatics 2020.
</p>
</li>
</ul>
<p></section>
</p>
<p><section class="card">
</p>
<h2 id="4"><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment Analysis.</a></h2>
<p><tt>#Multimodal, #NLP, #Speech and Audio, #Sentiment Analysis, #Feature Fusion</tt>
</p>
<table class="imgtable"><tr><td>
<img src="imgs/MSA.png" alt="MSA" width="300px" />&nbsp;</td>
<td align="left"><p><i>Sentiment analysis, mostly based on text, has been rapidly developing in the last decade and has attracted widespread attention in both academia and industry. However, the information in the real world usually comes from multiple modalities, such as audio and text. Therefore, in this paper, based on audio and text, we consider the task of multimodal sentiment analysis and propose a novel fusion strategy including both multi-feature fusion and multi-modality fusion to improve the accuracy of audio-text sentiment analysis. We call it the DFF-ATMF (Deep Feature Fusion - Audio and Text Modality Fusion) model, which consists of two parallel branches, the audio modality based branch and the text modality based branch. Its core mechanisms are the fusion of multiple feature vectors and multiple modality attention. Experiments on the CMU-MOSI dataset and the recently released CMU-MOSEI dataset, both collected from YouTube for sentiment analysis, show the very competitive results of our DFF-ATMF model. Furthermore, by virtue of attention weight distribution heatmaps, we also demonstrate the deep features learned by using DFF-ATMF are complementary to each other and robust. Surprisingly, DFF-ATMF also achieves new state-of-the-art results on the IEMOCAP dataset, indicating that the proposed fusion strategy also has a good generalization ability for multimodal emotion recognition.</i>
</p>
</td></tr></table>
<h3>Papers</h3>
<ul>
<li><p><b><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment Analysis.</a></b> 
<br /> 
<b>Feiyang Chen</b>, Ziqian Luo, Yanyan Xu and Dengfeng Ke. In Proceedings of the AAAI-20 Workshop on Affective Content Analysis, New York, USA, AAAI.
</p>
</li>
<li><p><b><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Audio-Text Sentiment Analysis using Deep Robust Complementary Fusion of Multi-Features and Multi-Modalities.</a></b> 
<br /> 
<b>Feiyang Chen</b>, Ziqian Luo. Accepted by NeurIPS 2019 Workshop: NewInML 2019, Vancouver, BC, Canada, NeurIPS.
</p>
</li>
<li><p><b><a href="http://ceur-ws.org/Vol-2328/3_2_paper_17.pdf" target=&ldquo;blank&rdquo;>Audio Sentiment Analysis by Heterogeneous Signal Features Learned from Utterance-Based Parallel Neural Network.</a></b> 
<br /> 
Ziqian Luo, Hua Xu and <b>Feiyang Chen</b>. In Proceedings of the AAAI-19 Workshop on Affective Content Analysis, Honolulu, USA, AAAI.
</p>
</li>
</ul>
<p></section>
</p>
<!-- <ul>
<li><p id="1"><a href="https://www.cell.com/" target=&ldquo;blank&rdquo;>Tumor2Graph: a novel Overall-Tumor-Profile-derived virtual graph deep learning for predicting tumor typing and subtyping.</a> 

<br /> In Submission, 2021.
</p>
</li>
<li><p><a href="https://www.nature.com/natmachintell/" target=&ldquo;blank&rdquo;>Predicting Antigen-Antibody Interaction via Global and Local Feature Deep Learning based on Primary Amino Acid Sequences.</a> <br /> Jie Zhang, Pengfei Zhou, Lu Lu, Mu Zhou, Jinru Ding, <b>Feiyang Chen</b>, Xuemei Zhang, Weifeng Wang, Shaoting Zhang. Submitted to <font color="red">Nature Machine Intelligence</font>, 2021.
</p>
</li>
<li><p><a href="https://arxiv.org/abs/2101.12547" target=&ldquo;blank&rdquo;>BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein Interactions.</a> <br /> Yifan Wu*, Min Gao*, Min Zeng, <b>Feiyang Chen</b>, Min Li, Jie Zhang. (*equal contribution). arXiv Preprint 2021.
</p>
</li>
<li><p><a href="https://www.mdpi.com/1999-4893/13/5/126/htm" target=&ldquo;blank&rdquo;>PUB-SalNet: A Pre-trained Unsupervised Self-Aware Backpropagation Network for Biomedical Salient Segmentation.</a> <br /> <b>Feiyang Chen*</b>, Ying Jiang*, Xiangrui Zeng and Min Xu. (*equal contribution). Algorithms 2020, 13(5), 126.
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment Analysis.</a> <br /> <b>Feiyang Chen</b>, Ziqian Luo, Yanyan Xu and Dengfeng Ke. In Proceedings of the AAAI-20 Workshop on Affective Content Analysis, New York, USA, AAAI.
</p>
</li>
<li><p><a href="https://pubmed.ncbi.nlm.nih.gov/32907544/" target=&ldquo;blank&rdquo;>A unified framework for packing deformable and non-deformable subcellular structures in crowded cryo-electron tomogram simulation.</a> <br /> Sinuo Liu, Xiaojuan Ban, Xiangrui Zeng, Fengnian Zhao, Yuan Gao, Wenjie Wu, Hongpan Zhang, <b>Feiyang Chen</b>, Thomas Hall, Xin Gao and Min Xu. BMC Bioinformatics 2020.
</p>
</li>
<li><p><a href="#" target=&ldquo;blank&rdquo;>Is Deep Learning All You Need for Unsupervised Saliency Detection of Biomedical Images?</a> <br /> <b>Feiyang Chen</b>, Ying Jiang, Xiangrui Zeng, and Min Xu. (Manuscript), 2019.
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1904.08138" target=&ldquo;blank&rdquo;>Audio-Text Sentiment Analysis using Deep Robust Complementary Fusion of Multi-Features and Multi-Modalities.</a> <br /> <b>Feiyang Chen</b>, Ziqian Luo. Accepted by NeurIPS 2019 Workshop: NewInML 2019, Vancouver, BC, Canada, NeurIPS.
</p>
</li>
<li><p><a href="http://ceur-ws.org/Vol-2328/3_2_paper_17.pdf" target=&ldquo;blank&rdquo;>Audio Sentiment Analysis by Heterogeneous Signal Features Learned from Utterance-Based Parallel Neural Network.</a> <br /> Ziqian Luo, Hua Xu and <b>Feiyang Chen</b>. In Proceedings of the AAAI-19 Workshop on Affective Content Analysis, Honolulu, USA, AAAI.
</p>
</li>
</ul> -->
<h2>Miscellaneous</h2>
<p>I have a super cute <a href="https://www.cs.cmu.edu/~yingj2/" target=&ldquo;blank&rdquo;>girlfriend</a> and we have had a wonderful time for three years (<a href="https://chenfeiyang.top/zb/" target=&ldquo;blank&rdquo;>live update</a>). <br />
I love astronomy and stargazing, and the <a href="https://en.wikipedia.org/wiki/The_Three-Body_Problem_(novel)" target=&ldquo;blank&rdquo;>Three-Body</a> is my favorite book. I am also a big fan of <a href="https://en.wikipedia.org/wiki/Sherlock_Holmes" target=&ldquo;blank&rdquo;>Sherlock Holmes</a>. That's where my GitHub avatar and username come from.
</p>
<div id="footer">
<div id="footer-text">
Page generated 2022-10-29, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
<!-- hitwebcounter Code START -->
<a href="https://www.hitwebcounter.com" target="_blank">
<img src="https://hitwebcounter.com/counter/counter.php?page=7990466&style=0027&nbdigits=5&type=page&initCount=10000" title="Free Counter" Alt="web counter"   border="0" /></a>              
</div>
</div>
</div>
</body>
</html>
